{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir relationGraph"
      ],
      "metadata": {
        "id": "9YkIxRS8N7vy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UijF2-neB_88",
        "outputId": "ac5c18a4-589b-4645-cd12-adb1b7a11357"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m81.9/86.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=f64e4e4f43e15dd419ab0d324a62e57bae0618ea0eafb8a385f608bfd1c64122\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference: https://huggingface.co/sentence-transformers"
      ],
      "metadata": {
        "id": "AOgMTjdgHKzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
      ],
      "metadata": {
        "id": "G5rxqoPgBwwv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = model.encode('job')\n",
        "passage_embedding = model.encode('Financial Analyst Job Opportunities - Google Search')\n",
        "\n",
        "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkj7j6BCCPpT",
        "outputId": "02c04d57-8f01-4100-d9a7-799313bcc810"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: tensor([[0.3890]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 0.3"
      ],
      "metadata": {
        "id": "fHYEYry_FTS4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = model.encode('course')\n",
        "passage_embedding = model.encode(\"financial analyst\")\n",
        "util.dot_score(query_embedding, passage_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tx1Q_LIOuyD",
        "outputId": "855d91ba-30cd-4da7-d913-abfc31642c83"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SentenceTransformer_match(person_id, key_word):\n",
        "  file_path = f\"./persona{person_id}.json\"\n",
        "  query_embedding = model.encode(key_word)\n",
        "  result = {\n",
        "      'browsingHistoryList' : [],\n",
        "      'facebookPostsList'   : [],\n",
        "      'schedule'            : [],\n",
        "      'info'                : []\n",
        "  }\n",
        "\n",
        "  with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)['data']\n",
        "\n",
        "    for bh in data['browsingHistoryList']:\n",
        "      passage_embedding = model.encode(bh['title'])\n",
        "      if util.dot_score(query_embedding, passage_embedding) > THRESHOLD:\n",
        "        result['browsingHistoryList'].append(bh['id'])\n",
        "\n",
        "    for pc in data['facebookPostsList']:\n",
        "      passage_embedding = model.encode(pc['content'])\n",
        "      if util.dot_score(query_embedding, passage_embedding) > THRESHOLD:\n",
        "        result['facebookPostsList'].append(pc['id'])\n",
        "\n",
        "    for sch in data['schedule']:\n",
        "      passage_embedding = model.encode(sch['address'])\n",
        "      if util.dot_score(query_embedding, passage_embedding) > THRESHOLD:\n",
        "        result['schedule'].append(sch['id'])\n",
        "\n",
        "    for info in data.keys():\n",
        "      if isinstance(data[info], str):\n",
        "        passage_embedding = model.encode(data[info])\n",
        "        if util.dot_score(query_embedding, passage_embedding) > THRESHOLD:\n",
        "          result['info'].append(info)\n",
        "\n",
        "  file_path = f\"./relationGraph/{person_id}_{key_word}.json\"\n",
        "  with open(file_path, 'w') as json_file:\n",
        "      json.dump(result, json_file)\n",
        "      print(f\"---------------Saved as {person_id}_{key_word}.json------------------\")\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "j1-DNuUCD03e"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def extract_pid_and_keyword(filename):\n",
        "  parts = filename.split('_')\n",
        "  if len(parts) == 2:\n",
        "    pid, keyword = parts\n",
        "    return pid, keyword.split('.')[0]\n",
        "  return None, None\n",
        "\n",
        "def search_files(directory):\n",
        "  matching_files = []\n",
        "  for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".json\"):\n",
        "      pid, keyword = extract_pid_and_keyword(filename)\n",
        "      if pid is not None and keyword is not None:\n",
        "        matching_files.append({\n",
        "            \"pid\": pid,\n",
        "            \"keyword\": keyword\n",
        "        })\n",
        "  return matching_files\n"
      ],
      "metadata": {
        "id": "du8sulZiQAIK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_for_existed_search(person_id, key_word):\n",
        "  matching_files_info = search_files(\"./relationGraph\")\n",
        "  query_embedding = model.encode(key_word)\n",
        "  res = []\n",
        "  for file_info in matching_files_info:\n",
        "    pid = int(file_info[\"pid\"])\n",
        "    keyword = file_info[\"keyword\"]\n",
        "    passage_embedding = model.encode(keyword)\n",
        "    if pid == person_id and util.dot_score(query_embedding, passage_embedding) > THRESHOLD:\n",
        "      res.append([pid, keyword])\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "s_Dj4_CARjgh"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_existed_search(person_id, key_word):\n",
        "  with open(f\"./relationGraph/{person_id}_{key_word}.json\", 'r') as file:\n",
        "    data = json.load(file)\n",
        "    return data"
      ],
      "metadata": {
        "id": "UDlS_j9sSyvD"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relevant_search(person_id, key_word):\n",
        "\n",
        "  existed_search = search_for_existed_search(person_id, key_word)\n",
        "  print(existed_search)\n",
        "  if len(existed_search) > 0:\n",
        "    print('Similar searches found:')\n",
        "    for i, s in enumerate(existed_search):\n",
        "      print(f\" {i+1}: pid: {s[0]}, keyword: {s[1]}\")\n",
        "    print('Do you want to view an existed search instead?')\n",
        "    user_input = int(input(\"If no, type 0; else, type index\"))\n",
        "    if user_input > 0:\n",
        "      return read_existed_search(person_id, existed_search[user_input-1][1])\n",
        "  return SentenceTransformer_match(person_id, key_word)\n",
        ""
      ],
      "metadata": {
        "id": "aBByisCpUfkF"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_search(2, 'dinner')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAQo7z32YoOj",
        "outputId": "2b0f24fd-59ba-415c-f0a0-00fb4aa46672"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 'lunch'], [2, 'eat'], [2, 'dinner']]\n",
            "Similar searches found:\n",
            " 1: pid: 2, keyword: lunch\n",
            " 2: pid: 2, keyword: eat\n",
            " 3: pid: 2, keyword: dinner\n",
            "Do you want to view an existed search instead?\n",
            "If no, type 0; else, type index2\n",
            "{'browsingHistoryList': [3, 6, 13, 14, 56, 125, 126, 127, 248, 249, 304, 306, 307, 317, 346, 354, 359, 397, 407, 417], 'facebookPostsList': ['2', '3', '6', '7', '8', '10', '11', '14', '15', '16', '19', '21', '22', '25', '26', '28', '29'], 'schedule': [1063], 'info': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_search(1, 'dinner')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYhBJWf9a26d",
        "outputId": "c8e9140d-a0d7-433f-aa5e-2199c6baf095"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 'dinner'], [1, 'eat']]\n",
            "Similar searches found:\n",
            " 1: pid: 1, keyword: dinner\n",
            " 2: pid: 1, keyword: eat\n",
            "Do you want to view an existed search instead?\n",
            "If no, type 0; else, type index0\n",
            "---------------Saved as 1_dinner.json------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'browsingHistoryList': [16,\n",
              "  63,\n",
              "  100,\n",
              "  105,\n",
              "  107,\n",
              "  116,\n",
              "  166,\n",
              "  171,\n",
              "  183,\n",
              "  186,\n",
              "  196,\n",
              "  197,\n",
              "  205,\n",
              "  209,\n",
              "  256,\n",
              "  260,\n",
              "  265,\n",
              "  266,\n",
              "  305,\n",
              "  306,\n",
              "  311,\n",
              "  314,\n",
              "  333,\n",
              "  338,\n",
              "  360],\n",
              " 'facebookPostsList': ['2',\n",
              "  '3',\n",
              "  '4',\n",
              "  '6',\n",
              "  '8',\n",
              "  '10',\n",
              "  '11',\n",
              "  '16',\n",
              "  '17',\n",
              "  '19',\n",
              "  '22'],\n",
              " 'schedule': [1001,\n",
              "  1004,\n",
              "  1006,\n",
              "  1012,\n",
              "  1015,\n",
              "  1017,\n",
              "  1023,\n",
              "  1026,\n",
              "  1029,\n",
              "  1033,\n",
              "  1036,\n",
              "  1039,\n",
              "  1044,\n",
              "  1047,\n",
              "  1050,\n",
              "  1054,\n",
              "  1058,\n",
              "  1063,\n",
              "  1067],\n",
              " 'info': ['industry']}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_dic(dic, person_id):\n",
        "  file_path = f\"./persona{person_id}.json\"\n",
        "  with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)['data']\n",
        "\n",
        "    for bh in data['browsingHistoryList']:\n",
        "      if bh['id'] in dic['browsingHistoryList']:\n",
        "        print(['browsingHistoryList', bh['id']], bh['title'])\n",
        "\n",
        "    for pc in data['facebookPostsList']:\n",
        "      if pc['id'] in dic['facebookPostsList']:\n",
        "        print(['facebookPostsList', pc['id']], pc['content'])\n",
        "\n",
        "    for sch in data['schedule']:\n",
        "      if sch['id'] in dic['schedule']:\n",
        "        print(['schedule', sch['id']], sch['address'])\n",
        "\n",
        "    for info in data.keys():\n",
        "      if isinstance(data[info], str):\n",
        "        if info in dic['info']:\n",
        "          print([info], data[info])"
      ],
      "metadata": {
        "id": "ph3ojCybcWQe"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = read_existed_search(2, 'lunch')\n",
        "decode_dic(dic, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwEOp0Dh72X",
        "outputId": "aa0221bb-5847-4342-8bb8-17964631e221"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['browsingHistoryList', 3] Healthy Breakfast Ideas - EatingWell\n",
            "['browsingHistoryList', 4] Morning Commute Tips - The Balance Careers\n",
            "['browsingHistoryList', 6] Healthy Lunch Ideas - BBC Good Food\n",
            "['browsingHistoryList', 13] Healthy Breakfast Ideas - Google Search\n",
            "['browsingHistoryList', 14] 30+ Healthy Breakfast Ideas for a Balanced Morning\n",
            "['browsingHistoryList', 56] Healthy Breakfast Recipes - 432 Birch St, Los Angeles, CA 90005\n",
            "['browsingHistoryList', 125] Breakfast Recipes - Google Search\n",
            "['browsingHistoryList', 126] Healthy Breakfast Ideas - Google Search\n",
            "['browsingHistoryList', 127] Breakfast Restaurants Near Me - Google Search\n",
            "['browsingHistoryList', 201] Breakfast and News Reading - 1200 S Central Ave, Los Angeles, CA 90021\n",
            "['browsingHistoryList', 206] Breakfast and News Reading - 1200 S Central Ave, Los Angeles, CA 90021\n",
            "['browsingHistoryList', 248] Quick and Healthy Breakfast Ideas - EatingWell\n",
            "['browsingHistoryList', 249] Easy Breakfast Recipes - Food Network\n",
            "['browsingHistoryList', 273] Breakfast and News Reading - News Headlines, Analysis & More | Reuters\n",
            "['browsingHistoryList', 304] Morning Exercise Tips\n",
            "['browsingHistoryList', 306] Healthy Breakfast Ideas\n",
            "['browsingHistoryList', 307] Breakfast Recipes - Easy and Quick\n",
            "['browsingHistoryList', 316] Morning Exercise Routine - YouTube\n",
            "['browsingHistoryList', 317] Healthy Breakfast Ideas - YouTube\n",
            "['browsingHistoryList', 318] 10 Tips for a Productive Workday - YouTube\n",
            "['browsingHistoryList', 354] Healthy Breakfast Recipes - EatingWell\n",
            "['browsingHistoryList', 359] Healthy Breakfast Ideas - Quick and Easy Recipes\n",
            "['browsingHistoryList', 397] How to Make a Healthy Breakfast - BBC Good Food\n",
            "['browsingHistoryList', 401] Morning Exercise - Google Search\n",
            "['browsingHistoryList', 407] Healthy Breakfast Ideas - Quick and Easy Recipes\n",
            "['browsingHistoryList', 417] Morning Exercise Tips\n",
            "['browsingHistoryList', 418] Morning Workout Routine at Home\n",
            "['facebookPostsList', '2'] Enjoying a delicious breakfast while catching up on the latest news. The perfect way to kickstart the day! #BreakfastTime #NewsUpdate\n",
            "['facebookPostsList', '3'] Taking a well-deserved lunch break. Fueling up with a healthy meal to recharge for the rest of the day! #LunchBreak #HealthyEating\n",
            "['facebookPostsList', '4'] Heading back home after a productive day at work. Time to relax and unwind! #CommuteHome #WorkLifeBalance\n",
            "['facebookPostsList', '7'] Enjoying a delicious breakfast while catching up on the latest news. It's important to stay informed and start the day right. #BreakfastTime #NewsReading\n",
            "['facebookPostsList', '8'] Taking a well-deserved lunch break to refuel and recharge. It's important to prioritize self-care and take time for oneself. #LunchBreak #SelfCare\n",
            "['facebookPostsList', '11'] Enjoying a delicious breakfast while catching up on the latest news. Ready to seize the day! #BreakfastTime #StayInformed\n",
            "['facebookPostsList', '14'] Taking a well-deserved lunch break. Refueling for the afternoon ahead! #LunchBreak #Foodie\n",
            "['facebookPostsList', '15'] Starting the day with a refreshing morning exercise. Keeping my body and mind in shape! #MorningExercise #HealthyLifestyle\n",
            "['facebookPostsList', '16'] Enjoying a delicious breakfast while catching up on the latest news. Ready to take on the day! üç≥üì∞ #BreakfastTime #NewsUpdate\n",
            "['facebookPostsList', '19'] Taking a well-deserved break and enjoying a delicious lunch. Re-energizing for the rest of the day! ü•™üòã #LunchBreak #FoodDelights\n",
            "['facebookPostsList', '21'] Enjoying a delicious breakfast while catching up on the latest news. Starting the day informed and fueled up! #BreakfastTime #StayInformed\n",
            "['facebookPostsList', '22'] Taking a well-deserved lunch break. Enjoying a satisfying meal and recharge for the rest of the day! #LunchBreak #WorkLifeBalance\n",
            "['facebookPostsList', '25'] Enjoying a delicious breakfast while catching up on the latest news. Ready to start the day! #BreakfastTime #StayInformed\n",
            "['facebookPostsList', '26'] Taking a well-deserved lunch break to refuel and recharge. Tasty meal and some relaxation time. #LunchBreak #MeTime\n",
            "['facebookPostsList', '27'] Starting my day with an energizing morning workout. Ready to take on the challenges ahead! üí™üèãÔ∏è‚Äç‚ôÄÔ∏è #FitnessGoals #HealthyLifestyle\n",
            "['facebookPostsList', '28'] Enjoying a delicious breakfast while catching up on the latest news headlines. Ready to stay informed and start the day right! üì∞‚òïÔ∏è #MorningRoutine #NewsUpdate\n",
            "['facebookPostsList', '29'] Taking a well-deserved break to enjoy a tasty lunch. Fueling up for an afternoon of productivity! ü•™ü•ó #LunchBreak #WorkLifeBalance\n",
            "['schedule', 1001] Breakfast and News Reading - 453 S Spring St, Los Angeles, CA 90013\n",
            "['schedule', 1004] Lunch Break - 515 S Flower St, Los Angeles, CA 90071\n",
            "['schedule', 1007] Dinner - 301 S Alvarado St, Los Angeles, CA 90057\n",
            "['schedule', 1012] Breakfast and News Reading - 432 Birch St, Los Angeles, CA 90005\n",
            "['schedule', 1015] Lunch Break - 678 Locust St, Los Angeles, CA 90005\n",
            "['schedule', 1018] Dinner - 123 Main St, Los Angeles, CA 90005\n",
            "['schedule', 1024] Breakfast and News Reading - 456 Elm St, Los Angeles, CA 90005\n",
            "['schedule', 1027] Lunch Break - 601 W 5th St, Los Angeles, CA 90071\n",
            "['schedule', 1030] Dinner Preparation - 789 Elm St, Los Angeles, CA 90005\n",
            "['schedule', 1035] Breakfast and News Reading - 1200 S Central Ave, Los Angeles, CA 90021\n",
            "['schedule', 1038] Lunch Break - 1100 Glendon Ave, Los Angeles, CA 90024\n",
            "['schedule', 1041] Dinner - 3500 W 6th St, Los Angeles, CA 90020\n",
            "['schedule', 1046] Breakfast and News Reading - 789 Elm St, Los Angeles, CA 90005\n",
            "['schedule', 1049] Lunch Break - 321 Walnut St, Los Angeles, CA 90005\n",
            "['schedule', 1052] Dinner - 123 Main St, Los Angeles, CA 90005\n",
            "['schedule', 1057] Breakfast and News Reading - 789 Oak Street, Los Angeles, CA 90005\n",
            "['schedule', 1060] Lunch Break - 234 Pine Street, Los Angeles, CA 90005\n",
            "['schedule', 1063] Dinner - 1220 Elm Street, Los Angeles, CA 90005\n",
            "['schedule', 1068] Breakfast and News Reading - 987 Hill St, Los Angeles, CA 90014\n",
            "['schedule', 1072] Lunch Break - 123 Flower St, Los Angeles, CA 90016\n",
            "['schedule', 1076] Dinner - 123 Main St, Los Angeles, CA 90001\n"
          ]
        }
      ]
    }
  ]
}